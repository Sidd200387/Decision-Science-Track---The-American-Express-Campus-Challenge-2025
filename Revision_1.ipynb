{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import duckdb\n",
    "\n",
    "os.chdir(r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\Revision\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "train_data_fp = r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\round 2 files\\train_data.parquet\"\n",
    "test_data_fp = r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\round 2 files\\test_data.parquet\"\n",
    "event_data_fp = r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\round 2 files\\add_event.parquet\"\n",
    "transaction_data_fp = r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\round 2 files\\add_trans.parquet\"\n",
    "offer_metadata_fp = r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\round 2 files\\offer_metadata.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files into dataframes\n",
    "train_data_df = pd.read_parquet(train_data_fp)\n",
    "test_data_df = pd.read_parquet(test_data_fp)\n",
    "event_data_df = pd.read_parquet(event_data_fp)\n",
    "transaction_data_df = pd.read_parquet(transaction_data_fp)\n",
    "offer_metadata_df = pd.read_parquet(offer_metadata_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for number of rows and columns\n",
    "print(train_data_df.shape)\n",
    "print(test_data_df.shape)\n",
    "print(event_data_df.shape)\n",
    "print(transaction_data_df.shape)\n",
    "print(offer_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f55dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4230a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5306fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all id columns\n",
    "\n",
    "def normalize_key_columns(df_dict, cols_to_normalize):\n",
    "    \"\"\"\n",
    "    Iterates through a dictionary of dataframes and normalizes\n",
    "    a specific list of columns (if they exist) by:\n",
    "    1. Converting to string\n",
    "    2. Stripping whitespace\n",
    "    3. Converting to lowercase\n",
    "    \"\"\"\n",
    "    print(\"--- Normalizing Key Columns ---\")\n",
    "    for df_name, df in df_dict.items():\n",
    "        # Get a list of columns that are in BOTH the df and our list\n",
    "        cols_in_df = [col for col in cols_to_normalize if col in df.columns]\n",
    "        \n",
    "        if not cols_in_df:\n",
    "            # Skip if this df has none of the key columns\n",
    "            continue\n",
    "\n",
    "        print(f\"Checking DataFrame: '{df_name}'. Normalizing: {cols_in_df}\")\n",
    "        \n",
    "        for col in cols_in_df:\n",
    "            # This .astype(str).str.strip().str.lower() chain\n",
    "            # is a robust way to clean key columns.\n",
    "            df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "            \n",
    "    print(\"--- Normalization Complete ---\")\n",
    "\n",
    "# --- Run this right after loading your data ---\n",
    "\n",
    "# 1. (You already have this) Load your DataFrames\n",
    "# train_data_df = pd.read_parquet(train_data_fp)\n",
    "# ...etc.\n",
    "\n",
    "# 2. Put them in the dictionary (like we did before)\n",
    "dataframes_to_check = {\n",
    "    'train': train_data_df,\n",
    "    'test': test_data_df,\n",
    "    'event': event_data_df,\n",
    "    'transaction': transaction_data_df,\n",
    "    'offer_metadata': offer_metadata_df\n",
    "}\n",
    "\n",
    "# 3. Define all your known key columns\n",
    "cols_to_normalize = ['id1', 'id2', 'id3', 'id4','id5','id6',\n",
    "                       'id7','id8','id9','id10','id11','id12','id13']\n",
    "\n",
    "# 4. Run the function\n",
    "normalize_key_columns(dataframes_to_check, cols_to_normalize)\n",
    "\n",
    "# Now, all your key columns are clean, and you can\n",
    "# proceed with your analysis and intersections confidently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40123399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "cols_to_check = ['id1', 'id2', 'id3', 'id4','id5','id6','id7','id8','id9','id10','id11','id12','id13']\n",
    "\n",
    "# --- Assuming your setup from the prompt ---\n",
    "# (You already have these lines)\n",
    "# cols_to_check = ['id1', 'id2', 'id3', 'id4','id5','id6','id7','id8','id9','id10','id11','id12','id13']\n",
    "# train_data_df = pd.read_parquet(train_data_fp)\n",
    "# test_data_df = pd.read_parquet(test_data_fp)\n",
    "# event_data_df = pd.read_parquet(event_data_fp)\n",
    "# transaction_data_df = pd.read_parquet(transaction_data_fp)\n",
    "# offer_metadata_df = pd.read_parquet(offer_metadata_fp)\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "## 1. Group DataFrames for easy checking\n",
    "# Use a dictionary to map a name to the DataFrame object\n",
    "dataframes_to_check = {\n",
    "    'train': train_data_df,\n",
    "    'test': test_data_df,\n",
    "    'event': event_data_df,\n",
    "    'transaction': transaction_data_df,\n",
    "    'offer_metadata': offer_metadata_df\n",
    "}\n",
    "\n",
    "## 2. Store findings in a dictionary\n",
    "# The key will be the column name (e.g., 'id1')\n",
    "# The value will be a set of all dtypes found for that column\n",
    "column_dtypes = defaultdict(set)\n",
    "\n",
    "## 3. Iterate and check\n",
    "print(\"Checking column data types...\")\n",
    "for col in cols_to_check:\n",
    "    for df_name, df in dataframes_to_check.items():\n",
    "        # This is the key: check if the column exists in the DataFrame\n",
    "        if col in df.columns:\n",
    "            # If it exists, add its dtype (as a string) to the set\n",
    "            column_dtypes[col].add(str(df[col].dtype))\n",
    "\n",
    "## 4. Report the results\n",
    "print(\"\\n--- Dtype Consistency Report ---\")\n",
    "inconsistent_cols = []\n",
    "\n",
    "for col, dtypes in column_dtypes.items():\n",
    "    if len(dtypes) == 0:\n",
    "        # This case means the column was in cols_to_check but not in any DataFrame\n",
    "        print(f\"ℹ️ {col:<15} | Not found in any dataset.\")\n",
    "    elif len(dtypes) == 1:\n",
    "        # Only one dtype was found, so it's consistent\n",
    "        print(f\"✅ {col:<15} | Consistent: {list(dtypes)[0]}\")\n",
    "    else:\n",
    "        # More than one dtype was found, this is an inconsistency\n",
    "        print(f\"⚠️ {col:<15} | INCONSISTENT: {dtypes}\")\n",
    "        inconsistent_cols.append(col)\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "if not inconsistent_cols:\n",
    "    print(\"All checked columns are consistent across datasets.\")\n",
    "else:\n",
    "    print(f\"Found {len(inconsistent_cols)} inconsistent columns: {inconsistent_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns present in all datasets\n",
    "train_data_df_cols = train_data_df.columns\n",
    "test_data_df_cols = test_data_df.columns\n",
    "event_data_df_cols = event_data_df.columns\n",
    "transaction_data_df_cols = transaction_data_df.columns\n",
    "offer_metadata_df_cols = offer_metadata_df.columns\n",
    "\n",
    "# Find different in train and test data\n",
    "print(\"Different columns in train and test data:\")\n",
    "for col in train_data_df_cols.difference(test_data_df_cols):\n",
    "    print(col)\n",
    "\n",
    "# Only 'y'-'clicked' was not present in the test data.\n",
    "\n",
    "# Find common columns in train and event data\n",
    "print(\"Common columns in train and event data:\")\n",
    "for col in train_data_df_cols.intersection(event_data_df_cols):\n",
    "    print(col)\n",
    "\n",
    "# Find common columns in train and transaction data\n",
    "print(\"Common columns in train and transaction data:\")\n",
    "for col in train_data_df_cols.intersection(transaction_data_df_cols):\n",
    "    print(col)\n",
    "\n",
    "# Find common columns in train and offer data\n",
    "print(\"Common columns in train and offer data:\")\n",
    "for col in train_data_df_cols.intersection(offer_metadata_df_cols):\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46092dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common id2 between train and event data\n",
    "common_id2 = set(train_data_df[\"id2\"]).intersection(event_data_df[\"id2\"])\n",
    "print(f\"Number of common id2s between train and event data: {len(common_id2)}\")\n",
    "\n",
    "# Find common id3 between train and event data\n",
    "common_id3 = set(train_data_df[\"id3\"]).intersection(event_data_df[\"id3\"])\n",
    "print(f\"Number of common id3s between train and event data: {len(common_id3)}\")\n",
    "\n",
    "# Find common id4 between train and event data\n",
    "common_id4 = set(train_data_df[\"id4\"]).intersection(event_data_df[\"id4\"])\n",
    "print(f\"Number of common id4s between train and event data: {len(common_id4)}\")\n",
    "\n",
    "# Find common id2 between test and event data\n",
    "common_id2 = set(test_data_df[\"id2\"]).intersection(event_data_df[\"id2\"])\n",
    "print(f\"Number of common id2s between test and event data: {len(common_id2)}\")\n",
    "\n",
    "# Find common id3 between test and event data\n",
    "common_id3 = set(test_data_df[\"id3\"]).intersection(event_data_df[\"id3\"])\n",
    "print(f\"Number of common id3s between test and event data: {len(common_id3)}\")\n",
    "\n",
    "# Find common id4 between test and event data\n",
    "common_id4 = set(test_data_df[\"id4\"]).intersection(event_data_df[\"id4\"])\n",
    "print(f\"Number of common id4s between test and event data: {len(common_id4)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a3b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common id2 between train and test data\n",
    "common_id2 = set(train_data_df[\"id2\"]).intersection(test_data_df[\"id2\"])\n",
    "print(f\"Number of common id2s between train and test data: {len(common_id2)}\")\n",
    "\n",
    "only_train_id2 = set(train_data_df[\"id2\"]).difference(common_id2)\n",
    "print(f\"Number of unique id2 in train and not in test:  {len(only_train_id2)}\")\n",
    "\n",
    "only_test_id2 = set(test_data_df[\"id2\"]).difference(common_id2)\n",
    "print(f\"Number of unique id2 in test and not in train:  {len(only_test_id2)}\")\n",
    "\n",
    "train_id2 = set(train_data_df[\"id2\"])\n",
    "print(f'Number of unique id2 in train: {len(train_id2)}')\n",
    "\n",
    "test_id2 = set(test_data_df[\"id2\"])\n",
    "print(f'Number of unique id2 in test: {len(test_id2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common id3 between train and test data\n",
    "common_id3 = set(train_data_df[\"id3\"]).intersection(test_data_df[\"id3\"])\n",
    "print(f\"Number of common id3s between train and test data: {len(common_id3)}\")\n",
    "\n",
    "only_train_id3 = set(train_data_df[\"id3\"]).difference(common_id3)\n",
    "print(f\"Number of unique id3 in train and not in test:  {len(only_train_id3)}\")\n",
    "\n",
    "only_test_id3 = set(test_data_df[\"id3\"]).difference(common_id3)\n",
    "print(f\"Number of unique id3 in test and not in train:  {len(only_test_id3)}\")\n",
    "\n",
    "train_id3 = set(train_data_df[\"id3\"])\n",
    "print(f'Number of unique id3 in train: {len(train_id3)}')\n",
    "\n",
    "test_id3 = set(test_data_df[\"id3\"])\n",
    "print(f'Number of unique id3 in test: {len(test_id3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11eecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common id4 between train and test data\n",
    "common_id4 = set(train_data_df[\"id4\"]).intersection(test_data_df[\"id4\"])\n",
    "print(f\"Number of common id4 between train and test data: {len(common_id4)}\")\n",
    "\n",
    "only_train_id4 = set(train_data_df[\"id4\"]).difference(common_id4)\n",
    "print(f\"Number of unique id4 in train and not in test:  {len(only_train_id4)}\")\n",
    "\n",
    "only_test_id4 = set(test_data_df[\"id4\"]).difference(common_id4)\n",
    "print(f\"Number of unique id4 in test and not in train:  {len(only_test_id4)}\")\n",
    "\n",
    "train_id4 = set(train_data_df[\"id4\"])\n",
    "print(f'Number of unique id4 in train: {len(train_id4)}')\n",
    "\n",
    "test_id4 = set(test_data_df[\"id4\"])\n",
    "print(f'Number of unique id4 in test: {len(test_id4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common id2 between train and transaction data\n",
    "common_id2_train_txn = set(train_data_df[\"id2\"]).intersection(transaction_data_df[\"id2\"])\n",
    "print(f\"Number of common id2s between train and transaction data: {len(common_id2_train_txn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caabae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common id2 between train and event data\n",
    "common_id2_train_event = set(train_data_df[\"id2\"]).intersection(event_data_df[\"id2\"])\n",
    "print(f\"Number of common id2s between train and event data: {len(common_id2_train_event)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common id2 between transaction and event data\n",
    "common_id2_transaction_event = set(transaction_data_df[\"id2\"]).intersection(event_data_df[\"id2\"])\n",
    "print(f\"Number of common id2s between transaction and event data: {len(common_id2_transaction_event)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38303c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common id3 between train and offer data\n",
    "common_id3_train_offer = set(train_data_df[\"id3\"]).intersection(offer_metadata_df[\"id3\"])\n",
    "print(f\"Number of common id3s between train and offer data: {len(common_id3_train_offer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54129d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common id2 between transaction and event data\n",
    "common_id2 = set(transaction_data_df[\"id2\"]).intersection(event_data_df[\"id2\"])\n",
    "print(f\"Number of common id2 between transaction and event data: {len(common_id2)}\")\n",
    "\n",
    "only_transaction_id2 = set(transaction_data_df[\"id2\"]).difference(common_id2)\n",
    "print(f\"Number of unique id2 in transaction and not in event:  {len(only_transaction_id2)}\")\n",
    "\n",
    "only_event_id2 = set(event_data_df[\"id2\"]).difference(common_id2)\n",
    "print(f\"Number of unique id2 in event and not in transaction:  {len(only_event_id2)}\")\n",
    "\n",
    "transaction_id2 = set(transaction_data_df[\"id2\"])\n",
    "print(f'Number of unique id2 in transaction: {len(transaction_id2)}')\n",
    "\n",
    "event_id2 = set(event_data_df[\"id2\"])\n",
    "print(f'Number of unique id2 in event: {len(event_id2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed4985",
   "metadata": {},
   "source": [
    "Progress till now\n",
    "- Merge offer with train and test\n",
    "- Merge transaction with event (do later if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29dff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge offer with train and test on id3\n",
    "train_data_merged_df = pd.merge(train_data_df, offer_metadata_df, how=\"left\", on='id3')\n",
    "test_data_merged_df = pd.merge(test_data_df, offer_metadata_df, how=\"left\", on='id3')\n",
    "\n",
    "print(train_data_merged_df.head())\n",
    "print(test_data_merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e912bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data integrity\n",
    "\n",
    "# Check for number of rows and columns\n",
    "print(train_data_merged_df.shape)\n",
    "print(test_data_merged_df.shape)\n",
    "\n",
    "# Get columns present in all datasets\n",
    "train_data_merged_df_cols = train_data_merged_df.columns\n",
    "test_data_merged_df_cols = test_data_merged_df.columns\n",
    "\n",
    "# Find different in train_merged and test_merged data\n",
    "print(\"Different columns in train_merged and test_merged data:\")\n",
    "for col in train_data_merged_df_cols.difference(test_data_merged_df_cols):\n",
    "    print(col)\n",
    "\n",
    "# Only 'y'-'clicked' was not present in the test_merged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\Revision\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82374f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Assuming your merged DataFrames are in memory ---\n",
    "# train_data_merged_df = pd.merge(train_data_df, offer_metadata_df, how=\"left\", on='id3')\n",
    "# test_data_merged_df = pd.merge(test_data_df, offer_metadata_df, how=\"left\", on='id3')\n",
    "\n",
    "# Define your output filepaths\n",
    "train_output_fp = 'train_data_merged.parquet'\n",
    "test_output_fp = 'test_data_merged.parquet'\n",
    "\n",
    "print(f\"Saving merged training data to {train_output_fp}...\")\n",
    "# Use .to_parquet() to save\n",
    "# index=False is important to avoid saving the pandas index as a separate column\n",
    "train_data_merged_df.to_parquet(train_output_fp, index=False)\n",
    "\n",
    "print(f\"Saving merged test data to {test_output_fp}...\")\n",
    "test_data_merged_df.to_parquet(test_output_fp, index=False)\n",
    "\n",
    "print(\"Save complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d5568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_merged_df['f285'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ee1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is loaded into a variable named 'train_data_merged_df'\n",
    "# Example (uncomment and use if you need to load it):\n",
    "# train_data_merged_df = pd.read_csv('your_file_name.csv')\n",
    "\n",
    "# Find columns where all values are NaN\n",
    "all_nan_cols = train_data_merged_df.columns[train_data_merged_df.isnull().all()]\n",
    "\n",
    "# Get the count of such columns\n",
    "num_all_nan_cols = len(all_nan_cols)\n",
    "\n",
    "# Print the results\n",
    "if num_all_nan_cols > 0:\n",
    "    print(f\"Found {num_all_nan_cols} columns with only NaN values.\")\n",
    "    print(\"These columns are:\")\n",
    "    \n",
    "    # Print the list of column names\n",
    "    for col_name in all_nan_cols:\n",
    "        print(col_name)\n",
    "else:\n",
    "    print(\"No columns were found with only NaN values.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
