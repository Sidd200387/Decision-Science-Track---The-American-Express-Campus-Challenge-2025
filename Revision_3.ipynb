{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ffe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import duckdb\n",
    "%matplotlib inline\n",
    "import plotly.express as px \n",
    "\n",
    "# os.chdir(r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\Revision\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload filepaths\n",
    "train_data_fp = r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\Revision\\train_data_cols_sorted.parquet\"\n",
    "test_data_fp = r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\Revision\\test_data_cols_sorted.parquet\"\n",
    "\n",
    "# Load files into dataframes\n",
    "train_data_df = pd.read_parquet(train_data_fp)\n",
    "test_data_df = pd.read_parquet(test_data_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfed3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for number of rows and columns\n",
    "print(train_data_df.shape)\n",
    "print(test_data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a85a34",
   "metadata": {},
   "source": [
    "# Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc0d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is loaded into a variable named 'train_data_df'\n",
    "# Example (uncomment and use if you need to load it):\n",
    "# train_data_df = pd.read_csv('your_file_name.csv')\n",
    "\n",
    "# Find columns where all values are NaN\n",
    "all_nan_cols = train_data_df.columns[train_data_df.isnull().all()]\n",
    "\n",
    "# Get the count of such columns\n",
    "num_all_nan_cols = len(all_nan_cols)\n",
    "\n",
    "# Print the results\n",
    "if num_all_nan_cols > 0:\n",
    "    print(f\"Found {num_all_nan_cols} columns with only NaN values.\")\n",
    "    print(\"These columns are:\")\n",
    "    \n",
    "    # Print the list of column names\n",
    "    for col_name in all_nan_cols:\n",
    "        print(col_name)\n",
    "else:\n",
    "    print(\"No columns were found with only NaN values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e127e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with all missing values\n",
    "train_data_df.drop(columns=all_nan_cols, inplace=True)\n",
    "test_data_df.drop(columns=all_nan_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_single_value_cols(df, df_name):\n",
    "    \"\"\"\n",
    "    Finds columns in a DataFrame that have only one unique non-NaN value.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Checking {df_name} ---\")\n",
    "    \n",
    "    # This list will hold the names of columns that match the criteria\n",
    "    single_value_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # .nunique() by default only counts non-null unique values.\n",
    "        # So, if this is 1, it means the column has exactly one \n",
    "        # unique value plus any number of NaNs.\n",
    "        if df[col].nunique() == 1:\n",
    "            single_value_cols.append(col)\n",
    "            \n",
    "    if not single_value_cols:\n",
    "        print(\"No columns found with only one unique value.\")\n",
    "    else:\n",
    "        print(f\"Found {len(single_value_cols)} columns with only one unique value:\")\n",
    "        print(single_value_cols)\n",
    "        \n",
    "        # Optional: Print the unique value for confirmation\n",
    "        print(\"\\nUnique values in these columns:\")\n",
    "        for col in single_value_cols:\n",
    "            # .dropna() removes NaNs, .unique() finds the one value\n",
    "            unique_val = df[col].dropna().unique()[0]\n",
    "            print(f\"  {col:<10} | {unique_val}\")\n",
    "            \n",
    "    return single_value_cols\n",
    "\n",
    "# --- Assuming train_data_df and test_data_df are loaded ---\n",
    "\n",
    "# Find single-value columns in the training data\n",
    "train_single_val = find_single_value_cols(train_data_df, 'train_data_df')\n",
    "\n",
    "# Find single-value columns in the test data\n",
    "#test_single_val = find_single_value_cols(test_data_df, 'test_data_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df[['f20']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with only single values or missing\n",
    "train_data_df.drop(columns=train_single_val, inplace=True)\n",
    "test_data_df.drop(columns=train_single_val, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141354d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_single_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82073ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Your train_data_df is already loaded\n",
    "# train_data_df = pd.read_parquet(...) \n",
    "\n",
    "output_excel_fp = 'high_missing_cols_report.xlsx'\n",
    "# ---------------------\n",
    "\n",
    "try:\n",
    "    print(\"Analyzing 'train_data_df'...\")\n",
    "    \n",
    "    # 1. Calculate Missing %\n",
    "    print(\"Calculating missing value percentages...\")\n",
    "    total_rows = len(train_data_df)\n",
    "    missing_percent = (train_data_df.isnull().sum() / total_rows)\n",
    "    \n",
    "    # 2. Filter Columns to find those > 95% missing\n",
    "    high_missing_cols = missing_percent[missing_percent > 0.95].index\n",
    "    \n",
    "    if len(high_missing_cols) == 0:\n",
    "        print(\"No columns found with more than 95% missing values.\")\n",
    "    else:\n",
    "        print(f\"Found {len(high_missing_cols)} columns with > 95% missing values.\")\n",
    "        \n",
    "        # 3. Prepare 'y' for correlation\n",
    "        if 'y' in train_data_df.columns:\n",
    "            y_numeric = train_data_df['y'].astype(float)\n",
    "            report_data = []\n",
    "            \n",
    "            # 4. Calculate Correlations\n",
    "            print(\"Calculating correlations with 'y'...\")\n",
    "            for col in high_missing_cols:\n",
    "                col_type = train_data_df[col].dtype\n",
    "                correlation = np.nan \n",
    "                \n",
    "                # --- THIS IS THE FIXED LINE ---\n",
    "                # Changed from pd.api.types.is_categorical_dtype(col_type)\n",
    "                if isinstance(col_type, pd.CategoricalDtype):\n",
    "                # ------------------------------\n",
    "                    col_numeric = train_data_df[col].cat.codes.replace(-1, np.nan)\n",
    "                elif pd.api.types.is_numeric_dtype(col_type):\n",
    "                    col_numeric = train_data_df[col].astype(float)\n",
    "                else:\n",
    "                    col_numeric = None\n",
    "                \n",
    "                if col_numeric is not None:\n",
    "                    correlation = y_numeric.corr(col_numeric)\n",
    "                \n",
    "                # 5. Add data to our report list\n",
    "                report_data.append({\n",
    "                    'column_name': col,\n",
    "                    'missing_percentage': missing_percent[col] * 100,\n",
    "                    'correlation_with_y': correlation if col_numeric is not None else 'N/A (non-numeric)'\n",
    "                })\n",
    "            \n",
    "            # 6. Create and Export the Excel Report\n",
    "            report_df = pd.DataFrame(report_data)\n",
    "            report_df = report_df.sort_values(by='missing_percentage', ascending=False)\n",
    "            \n",
    "            print(f\"Exporting report to {output_excel_fp}...\")\n",
    "            report_df.to_excel(output_excel_fp, index=False)\n",
    "            \n",
    "            print(\"\\n--- Report Summary ---\")\n",
    "            print(report_df.to_string())\n",
    "            print(f\"\\n✅ Successfully created and exported '{output_excel_fp}'.\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Error: 'y' column not found in the training data.\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"Error: 'train_data_df' is not loaded in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90584fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76260ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with only more than 95% missing values\n",
    "train_data_df.drop(columns=high_missing_cols, inplace=True)\n",
    "test_data_df.drop(columns=high_missing_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a18f5",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ea952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find class distribution of y\n",
    "\n",
    "train_data_df[['y']].value_counts()   # highly imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b42f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution of y\n",
    "x=train_data_df.drop([\"y\"],axis=1)\n",
    "y=train_data_df[\"y\"]\n",
    "\n",
    "count_class = y.value_counts() # Count the occurrences of each class\n",
    "plt.bar(count_class.index, count_class.values)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(count_class.index, ['Class 0', 'Class 1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis: Check the Account Creation Indicator with y\n",
    "\n",
    "train_data_df[['f50']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_df is loaded\n",
    "\n",
    "print(\"Checking condition: IF f50 == 'N' THEN y == 0\")       # Some people even click for offers even if they don't have Amex account.\n",
    "\n",
    "# 1. Filter the DataFrame to get only rows where f50 is 'N'\n",
    "f50_is_N = train_data_df[train_data_df['f50'] == 'N']\n",
    "\n",
    "# 2. Check the value counts of 'y' *within that filtered set*\n",
    "y_counts_when_f50_is_N = f50_is_N['y'].value_counts()\n",
    "\n",
    "print(\"\\nValue counts of 'y' when 'f50' is 'N':\")\n",
    "print(y_counts_when_f50_is_N)\n",
    "\n",
    "# 3. Report the result\n",
    "# We check if the key '1' (or 1.0) exists in the value counts\n",
    "if 1 in y_counts_when_f50_is_N.index or '1' in y_counts_when_f50_is_N.index:\n",
    "    count = y_counts_when_f50_is_N.get(1, 0) or y_counts_when_f50_is_N.get('1', 0)\n",
    "    print(f\"\\n❌ FAILED: Found {count} rows where f50 is 'N' but y is 1.\")\n",
    "else:\n",
    "    print(\"\\n✅ PASSED: All rows where f50 is 'N' have y as 0 (or missing).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fe394",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df[['f52']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ed3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_data_df is loaded\n",
    "\n",
    "print(\"Checking condition: IF f52 == 'N' THEN y == 0\")         # People click even if they are not active members\n",
    "\n",
    "# 1. Filter the DataFrame to get only rows where f52 is 'N'\n",
    "f52_is_N = train_data_df[train_data_df['f52'] == 'N']\n",
    "\n",
    "# 2. Check the value counts of 'y' *within that filtered set*\n",
    "y_counts_when_f52_is_N = f52_is_N['y'].value_counts()\n",
    "\n",
    "print(\"\\nValue counts of 'y' when 'f52' is 'N':\")\n",
    "print(y_counts_when_f52_is_N)\n",
    "\n",
    "# 3. Report the result\n",
    "# We check if the key '1' (or 1.0) exists in the value counts\n",
    "if 1 in y_counts_when_f52_is_N.index or '1' in y_counts_when_f52_is_N.index:\n",
    "    count = y_counts_when_f52_is_N.get(1, 0) or y_counts_when_f52_is_N.get('1', 0)\n",
    "    print(f\"\\n❌ FAILED: Found {count} rows where f52 is 'N' but y is 1.\")\n",
    "else:\n",
    "    print(\"\\n✅ PASSED: All rows where f52 is 'N' have y as 0 (or missing).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b75d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all non-account holders are not active.\n",
    "# Assuming train_data_df is loaded\n",
    "\n",
    "print(\"Checking condition: IF f50 == 'N' THEN y == 0\")       # Some people even click for offers even if they don't have Amex account.\n",
    "\n",
    "# 1. Filter the DataFrame to get only rows where f50 is 'N'\n",
    "f50_is_N = train_data_df[train_data_df['f50'] == 'N']\n",
    "\n",
    "# 2. Check the value counts of 'y' *within that filtered set*\n",
    "y_counts_when_f50_is_N = f50_is_N['y'].value_counts()\n",
    "\n",
    "print(\"\\nValue counts of 'y' when 'f50' is 'N':\")\n",
    "print(y_counts_when_f50_is_N)\n",
    "\n",
    "# 3. Report the result\n",
    "# We check if the key '1' (or 1.0) exists in the value counts\n",
    "if 1 in y_counts_when_f50_is_N.index or '1' in y_counts_when_f50_is_N.index:\n",
    "    count = y_counts_when_f50_is_N.get(1, 0) or y_counts_when_f50_is_N.get('1', 0)\n",
    "    print(f\"\\n❌ FAILED: Found {count} rows where f50 is 'N' but y is 1.\")\n",
    "else:\n",
    "    print(\"\\n✅ PASSED: All rows where f50 is 'N' have y as 0 (or missing).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_data_df is loaded\n",
    "\n",
    "# --- Check 1: f52 is never 'Y' whenever f50 is 'N' ---\n",
    "print(\"Checking condition: IF f50 == 'N' THEN f52 != 'Y'\")\n",
    "\n",
    "# 1. Filter for rows where f50 is 'N'\n",
    "f50_is_N = train_data_df[train_data_df['f50'] == 'N']\n",
    "\n",
    "# 2. Check the value counts of 'f52' within that filtered set\n",
    "f52_counts_when_f50_is_N = f50_is_N['f52'].value_counts()\n",
    "\n",
    "print(\"\\nValue counts of 'f52' when 'f50' is 'N':\")\n",
    "print(f52_counts_when_f50_is_N)\n",
    "\n",
    "# 3. Report the result (Check for the violating condition, 'Y')\n",
    "if 'Y' in f52_counts_when_f50_is_N.index:\n",
    "    count = f52_counts_when_f50_is_N['Y']\n",
    "    print(f\"\\n❌ FAILED: Found {count} rows where f50 is 'N' but f52 is 'Y'.\")\n",
    "else:\n",
    "    print(\"\\n✅ PASSED: All rows where f50 is 'N' have f52 as not 'Y'.\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\") # Separator\n",
    "\n",
    "\n",
    "# --- Check 2: f50 is 'Y' whenever f52 is 'Y' ---\n",
    "print(\"Checking condition: IF f52 == 'Y' THEN f50 == 'Y'\")\n",
    "\n",
    "# 1. Filter for rows where f52 is 'Y'\n",
    "f52_is_Y = train_data_df[train_data_df['f52'] == 'Y']\n",
    "\n",
    "# 2. Check the value counts of 'f50' within that filtered set\n",
    "f50_counts_when_f52_is_Y = f52_is_Y['f50'].value_counts()\n",
    "\n",
    "print(\"\\nValue counts of 'f50' when 'f52' is 'Y':\")\n",
    "print(f50_counts_when_f52_is_Y)\n",
    "\n",
    "# 3. Report the result (Check for the violating condition, 'N' or any non-'Y')\n",
    "# We check if 'N' (the most likely alternative) exists\n",
    "if 'N' in f50_counts_when_f52_is_Y.index:\n",
    "    count = f50_counts_when_f52_is_Y['N']\n",
    "    print(f\"\\n❌ FAILED: Found {count} rows where f52 is 'Y' but f50 is 'N'.\")\n",
    "# You could also check for any value *other* than 'Y'\n",
    "elif len(f50_counts_when_f52_is_Y) > 1 or ('Y' not in f50_counts_when_f52_is_Y.index and len(f50_counts_when_f52_is_Y) > 0):\n",
    "     print(f\"\\n❌ FAILED: Found rows where f52 is 'Y' but f50 is not 'Y'.\")\n",
    "else:\n",
    "    print(\"\\n✅ PASSED: All rows where f52 is 'Y' also have f50 as 'Y'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clicks and impressions in last 1 day\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "temp_df = train_data_df.sample(100000)\n",
    "sns.scatterplot(x=temp_df.f320, y= temp_df.f315, alpha=0.8)\n",
    "plt.title('Clicks vs Impression in last 1 day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6236f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clicks and impressions in last 1 day\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "temp_df = train_data_df.sample(100000)\n",
    "sns.scatterplot(x=temp_df.f315, y= temp_df.f320, alpha=0.8)\n",
    "plt.title('Clicks vs Impression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day, month, day of week, from date\n",
    "\n",
    "train_data_df['id5'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c13405",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df[['f349', 'f350']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb187eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df[['f231','f237','f252','f269']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b20349",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df[['f78','f81']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea7327e",
   "metadata": {},
   "source": [
    "# Strategy: Remove anything (any feature) which is more than 30 days old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['f38', 'f68', 'f69', 'f71', 'f72', 'f73', 'f74', 'f75', 'f82', 'f83', 'f87', 'f90', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f121', 'f163', 'f164', 'f165', 'f166', 'f167', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f186', 'f198', 'f361', 'f362', 'f363']\n",
    "\n",
    "\n",
    "train_data_df = train_data_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# CRITICAL: Also drop from the official test set\n",
    "test_data_df = test_data_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "print(train_data_df.shape)\n",
    "print(test_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645853af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns related to the ratio of pages viewed variable.\n",
    "cols_to_drop = ['f78', 'f81', 'f85', 'f89']\n",
    "\n",
    "train_data_df = train_data_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# CRITICAL: Also drop from the official test set\n",
    "test_data_df = test_data_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "print(train_data_df.shape)\n",
    "print(test_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb7fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df[['id4']].head()\n",
    "train_data_df['id4'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0688983",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification Step ---\n",
    "print(\"--- Verifying 'id4' in train_data_df ---\")\n",
    "# We check if any timestamp is different from its 'normalized' (midnight) version.\n",
    "has_time_components_train = (train_data_df['id4'] != train_data_df['id4'].dt.normalize()).any()\n",
    "\n",
    "if has_time_components_train:\n",
    "    print(\"✅ PASSED: 'id4' in train_data_df contains time components (hours, minutes, seconds).\")\n",
    "    print(\"\\nHour distribution (Train):\")\n",
    "    print(train_data_df['id4'].dt.hour.value_counts().sort_index())\n",
    "else:\n",
    "    print(\"⚠️ FAILED: 'id4' in train_data_df only contains dates (all times are 00:00:00).\")\n",
    "\n",
    "print(\"\\n--- Verifying 'id4' in test_data_df ---\")\n",
    "has_time_components_test = (test_data_df['id4'] != test_data_df['id4'].dt.normalize()).any()\n",
    "\n",
    "if has_time_components_test:\n",
    "    print(\"✅ PASSED: 'id4' in test_data_df contains time components (hours, minutes, seconds).\")\n",
    "    print(\"\\nHour distribution (Test):\")\n",
    "    #print(test_data_df['id4'].dt.hour.value_counts().sort_index())\n",
    "else:\n",
    "    print(\"⚠️ FAILED: 'id4' in test_data_df only contains dates (all times are 00:00:00).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d615a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Extraction Step ---\n",
    "# Assuming the verification above passed, or you want to extract features anyway\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"--- Extracting all features from 'id4' ---\")\n",
    "\n",
    "try:\n",
    "    # --- Train data ---\n",
    "    print(\"Extracting features from train_data_df...\")\n",
    "    train_data_df['DayofMonth'] = train_data_df['id4'].dt.day\n",
    "    train_data_df['Month'] = train_data_df['id4'].dt.month\n",
    "    train_data_df['Year'] = train_data_df['id4'].dt.year\n",
    "    train_data_df['DayofWeek'] = train_data_df['id4'].dt.dayofweek\n",
    "    train_data_df['is_weekend'] = (train_data_df['id4'].dt.dayofweek >= 5).astype(int)\n",
    "    train_data_df['WeekofYear'] = train_data_df['id4'].dt.isocalendar().week\n",
    "    train_data_df['DayName'] = train_data_df['id4'].dt.day_name()\n",
    "    train_data_df['Hour'] = train_data_df['id4'].dt.hour\n",
    "\n",
    "    # --- Test data ---\n",
    "    print(\"\\nExtracting features from test_data_df...\")\n",
    "    test_data_df['DayofMonth'] = test_data_df['id4'].dt.day\n",
    "    test_data_df['Month'] = test_data_df['id4'].dt.month\n",
    "    test_data_df['Year'] = test_data_df['id4'].dt.year\n",
    "    test_data_df['DayofWeek'] = test_data_df['id4'].dt.dayofweek\n",
    "    test_data_df['is_weekend'] = (test_data_df['id4'].dt.dayofweek >= 5).astype(int)\n",
    "    test_data_df['WeekofYear'] = test_data_df['id4'].dt.isocalendar().week\n",
    "    test_data_df['DayName'] = test_data_df['id4'].dt.day_name()\n",
    "    test_data_df['Hour'] = test_data_df['id4'].dt.hour\n",
    "\n",
    "    print(\"\\n✅ Feature extraction complete.\")\n",
    "    print(\"Example of new columns in train_data_df:\")\n",
    "    print(train_data_df[['id4', 'DayName', 'Hour', 'Minute', 'is_weekend']].head())\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"\\n❌ ERROR: 'id4' column is not in datetime format.\")\n",
    "    print(f\"Please convert it first using:\")\n",
    "    print(\"train_data_df['id4'] = pd.to_datetime(train_data_df['id4'])\")\n",
    "    print(\"test_data_df['id4'] = pd.to_datetime(test_data_df['id4'])\")\n",
    "except KeyError as e:\n",
    "    print(f\"\\n❌ ERROR: A column was not found. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bf0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('hour distribution in train')\n",
    "#train_data_df['id4'].dt.hour.value_counts().sort_index()\n",
    "\n",
    "# Hourly distribution for all observations\n",
    "hour_counts = train_data_df['id4'].dt.hour.value_counts().sort_index()\n",
    "\n",
    "# Hourly distribution where 'y' == 1\n",
    "hour_y1_counts = train_data_df[train_data_df['y'] == 1]['id4'].dt.hour.value_counts().sort_index()\n",
    "\n",
    "# Combine into a single DataFrame for comparison\n",
    "hour_summary = pd.DataFrame({\n",
    "    'total_count': hour_counts,\n",
    "    'y1_count': hour_y1_counts\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "print(hour_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d740f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7fae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly distribution for all observations\n",
    "hour_counts = train_data_df['Hour'].value_counts().sort_index()\n",
    "\n",
    "# Hourly distribution where 'y' == 1\n",
    "hour_y1_counts = train_data_df[train_data_df['y'] == 1]['Hour'].value_counts().sort_index()\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "hour_summary = pd.DataFrame({\n",
    "    'No. of obs in that hour': hour_counts,\n",
    "    'Clicks': hour_y1_counts\n",
    "}).fillna(0).astype(int)                  # Clicks\n",
    "\n",
    "# Compute total number of y==1 for contribution percent\n",
    "total_y1 = hour_summary['Clicks'].sum()\n",
    "\n",
    "# Add required columns\n",
    "hour_summary['Ratio of clicks per obs'] = hour_summary['Clicks'] / hour_summary['No. of obs in that hour']\n",
    "hour_summary[f'% of clicks out of total clicks'] = (hour_summary['Clicks'] / total_y1) * 100\n",
    "\n",
    "print(hour_summary)\n",
    "\n",
    "hour_summary.to_excel('Clicks and hour.xlsx', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d17b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Define the buckets based on your data analysis\n",
    "# These lists hold the hours for each category we found\n",
    "late_night_hours = [0, 1, 2, 3]\n",
    "non_late_night_hours = [6, 8, 12, 17, 21, 22, 4, 7, 9, 10, 14, 16, 18, 19, 20, 5, 11, 13, 15, 23]\n",
    "\n",
    "# 2. Create a function to apply these buckets\n",
    "def assign_hour_bucket(hour):\n",
    "    if hour in late_night_hours:\n",
    "        return 'Late_Night'\n",
    "    elif hour in non_late_night_hours:\n",
    "        return 'Non_Late_Night'\n",
    "    else:\n",
    "        return 'Other_hrs' # As a fallback, though all hours (0-23) should be covered\n",
    "\n",
    "# 3. Apply the function to create the new feature\n",
    "print(\"Applying buckets to train and test data...\")\n",
    "train_data_df['Hour_Bucket'] = train_data_df['Hour'].apply(assign_hour_bucket)\n",
    "test_data_df['Hour_Bucket'] = test_data_df['Hour'].apply(assign_hour_bucket)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "# 4. Check the result\n",
    "print(\"\\nNew 'Hour_Bucket' value counts in training data:\")\n",
    "print(train_data_df['Hour_Bucket'].value_counts())\n",
    "\n",
    "print(\"\\nCrosstab of Hour_Bucket vs. Clicks (y):\")\n",
    "print(pd.crosstab(train_data_df['Hour_Bucket'], train_data_df['y'], normalize='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df[['DayofMonth', 'Month', 'Year', 'DayofWeek', 'is_weekend', 'WeekofYear', 'DayName', 'Hour', 'Hour_Bucket']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df[['DayofMonth', 'Month', 'Year', 'DayofWeek', 'is_weekend', 'WeekofYear', 'DayName', 'Hour', 'Hour_Bucket']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47edec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dummies for 'Hour_Bucket' and drop the first category ('Late_Night')\n",
    "# # This leaves you with just one new column, e.g., 'Hour_Bucket_Non_Late_Night'\n",
    "# train_data_df = pd.get_dummies(train_data_df, \n",
    "#                                columns=['Hour_Bucket'], \n",
    "#                                drop_first=True, \n",
    "#                                dtype=int)\n",
    "\n",
    "# test_data_df = pd.get_dummies(test_data_df, \n",
    "#                               columns=['Hour_Bucket'], \n",
    "#                               drop_first=True, \n",
    "#                               dtype=int)\n",
    "\n",
    "# # --- How to interpret the new column ---\n",
    "# # If 'Hour_Bucket_Non_Late_Night' == 1, it was 'Non_Late_Night'\n",
    "# # If 'Hour_Bucket_Non_Late_Night' == 0, it was 'Late_Night'\n",
    "\n",
    "# print(train_data_df.head())\n",
    "\n",
    "# 1. Create the new column based on your desired logic\n",
    "# This creates a boolean (True/False) and .astype(int) converts it to (1/0)\n",
    "train_data_df['Late_Night'] = (train_data_df['Hour_Bucket'] == 'Late_Night').astype(int)\n",
    "test_data_df['Late_Night'] = (test_data_df['Hour_Bucket'] == 'Late_Night').astype(int)\n",
    "\n",
    "# 2. Drop the original 'Hour_Bucket' column\n",
    "train_data_df = train_data_df.drop('Hour_Bucket', axis=1)\n",
    "test_data_df = test_data_df.drop('Hour_Bucket', axis=1)\n",
    "\n",
    "# --- How to interpret the new column ---\n",
    "# If 'Late_Night' == 1, it was 'Late_Night'\n",
    "# If 'Late_Night' == 0, it was 'Non_Late_Night'\n",
    "\n",
    "print(train_data_df.head())\n",
    "print(train_data_df['Late_Night'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93791639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that train data is not on weekend\n",
    "print(train_data_df[['WeekofYear']].value_counts())\n",
    "print(test_data_df[['WeekofYear']].value_counts())\n",
    "\n",
    "print(train_data_df[['is_weekend']].value_counts())\n",
    "print(test_data_df[['is_weekend']].value_counts())\n",
    "\n",
    "print(train_data_df[['Month']].value_counts())\n",
    "print(test_data_df[['Month']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ad185",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_df[['Year']].value_counts())\n",
    "print(test_data_df[['Year']].value_counts())\n",
    "\n",
    "print(train_data_df[['DayofWeek']].value_counts())\n",
    "print(test_data_df[['DayofWeek']].value_counts())\n",
    "\n",
    "print(train_data_df[['DayName']].value_counts())\n",
    "print(test_data_df[['DayName']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_df[['DayName']].value_counts())\n",
    "print(test_data_df[['DayName']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_df.shape)\n",
    "print(test_data_df.shape)\n",
    "# Remove unnecessary columns (Just kept the DayofWeek to make the train and val dfs in the end)\n",
    "date_cols_to_drop = ['DayofMonth', 'Month', 'Year', 'is_weekend', 'WeekofYear','DayName']\n",
    "train_data_df = train_data_df.drop(columns=date_cols_to_drop, errors='ignore')\n",
    "test_data_df = test_data_df.drop(columns=date_cols_to_drop, errors='ignore')\n",
    "print(train_data_df.shape)\n",
    "print(test_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns which are 90+ days older\n",
    "\n",
    "date_cols_to_drop = ['f36']\n",
    "train_data_df = train_data_df.drop(columns=date_cols_to_drop, errors='ignore')\n",
    "test_data_df = test_data_df.drop(columns=date_cols_to_drop, errors='ignore')\n",
    "print(train_data_df.shape)\n",
    "print(test_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a02940",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_df.info())\n",
    "print(test_data_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assuming your merged DataFrames are in memory ---\n",
    "\n",
    "# Define your output filepaths\n",
    "train_output_fp = 'train_df_datesort.parquet'\n",
    "test_output_fp = 'test_df_datesort.parquet'\n",
    "\n",
    "print(f\"Saving training data to {train_output_fp}...\")\n",
    "# Use .to_parquet() to save\n",
    "# index=False is important to avoid saving the pandas index as a separate column\n",
    "train_data_df.to_parquet(train_output_fp, index=False)\n",
    "\n",
    "print(f\"Saving test data to {test_output_fp}...\")\n",
    "test_data_df.to_parquet(test_output_fp, index=False)\n",
    "\n",
    "\n",
    "print(\"Save complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd65c6f",
   "metadata": {},
   "source": [
    "# For last part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb819a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assuming train_data_df and test_data_df are loaded ---\n",
    "\n",
    "# 1. Create the boolean masks for the split\n",
    "# 'DayofWeek' 2 (Wed), 3 (Thu), 4 (Fri)\n",
    "train_indices = train_data_df['DayofWeek'].isin([2, 3])\n",
    "val_indices = train_data_df['DayofWeek'] == 4\n",
    "\n",
    "# 2. Create new train_df and val_df (with 'y' included)\n",
    "# We use .copy() to avoid a SettingWithCopyWarning\n",
    "train_df = train_data_df.loc[train_indices].copy()\n",
    "val_df = train_data_df.loc[val_indices].copy()\n",
    "\n",
    "print(f\"New Training Set (train_df) size: {len(train_df)}\")\n",
    "print(f\"Validation Set (val_df) size: {len(val_df)}\")\n",
    "\n",
    "\n",
    "# 3. Define the list of all date-related columns to drop\n",
    "# (I've corrected the missing comma in your list)\n",
    "date_cols_to_drop = ['DayofWeek']\n",
    "\n",
    "# 4. Drop these columns from all three sets\n",
    "print(\"\\nDropping date-related columns from all sets...\")\n",
    "\n",
    "# Using errors='ignore' is robust in case 'DayName' wasn't created\n",
    "train_df = train_df.drop(columns=date_cols_to_drop, errors='ignore')\n",
    "val_df = val_df.drop(columns=date_cols_to_drop, errors='ignore')\n",
    "\n",
    "# CRITICAL: Also drop from the official test set\n",
    "test_data_df = test_data_df.drop(columns=date_cols_to_drop, errors='ignore')\n",
    "train__data_df = train_data_df.drop(columns=date_cols_to_drop, errors='ignore')\n",
    "\n",
    "print(\"Date columns dropped.\")\n",
    "\n",
    "# 5. Verification (you can check the columns)\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"'y' column in train_df: {'y' in train_data_df.columns}\")\n",
    "print(f\"'y' column in train_df: {'y' in train_df.columns}\")\n",
    "print(f\"'y' column in val_df: {'y' in val_df.columns}\")\n",
    "print(f\"'DayofWeek' column in train_df: {'DayofWeek' in train_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d12f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70df17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d00416",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d907a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af088f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for extra cols\n",
    "\n",
    "# Get columns present in all datasets\n",
    "train_df_cols = train_df.columns\n",
    "val_df_cols = val_df.columns\n",
    "test_data_df_cols = test_data_df.columns\n",
    "\n",
    "# Find different in train and test data\n",
    "print(\"Different columns in train and test data:\")\n",
    "for col in train_df_cols.difference(test_data_df_cols):\n",
    "    print(col)\n",
    "\n",
    "# Only 'y'-'clicked' was not present in the test data.\n",
    "\n",
    "print(\"Different columns in val and test data:\")\n",
    "for col in val_df_cols.difference(test_data_df_cols):\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5aad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assuming your merged DataFrames are in memory ---\n",
    "\n",
    "# Define your output filepaths\n",
    "train_output_fp = 'train_df.parquet'\n",
    "val_output_fp = 'val_df.parquet'\n",
    "test_output_fp = 'test_df.parquet'\n",
    "train_and_val_fp = 'train_and_val_df.parquet'\n",
    "\n",
    "print(f\"Saving training data to {train_output_fp}...\")\n",
    "# Use .to_parquet() to save\n",
    "# index=False is important to avoid saving the pandas index as a separate column\n",
    "train_df.to_parquet(train_output_fp, index=False)\n",
    "\n",
    "print(f\"Saving val data to {val_output_fp}...\")\n",
    "val_df.to_parquet(val_output_fp, index=False)\n",
    "\n",
    "print(f\"Saving test data to {test_output_fp}...\")\n",
    "test_data_df.to_parquet(test_output_fp, index=False)\n",
    "\n",
    "print(f\"Saving test data to {train_and_val_fp}...\")\n",
    "train_data_df.to_parquet(train_and_val_fp, index=False)\n",
    "\n",
    "print(\"Save complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_df.shape)\n",
    "print(train_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
