{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903691e1",
   "metadata": {},
   "source": [
    "# 2-step ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13140591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc074cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_map7_eval(id2_arr, id3_arr, y_arr):\n",
    "    \"\"\"\n",
    "    Builds a closure for LightGBM evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def map7_eval(preds, dataset):\n",
    "        # dataset labels\n",
    "        true = y_arr\n",
    "        \n",
    "        # group by customer\n",
    "        df = pd.DataFrame({\n",
    "            'customer': id2_arr,\n",
    "            'offer': id3_arr,\n",
    "            'y': true,\n",
    "            'p': preds\n",
    "        })\n",
    "\n",
    "        def apk(actual, predicted, k=7):\n",
    "            if len(predicted) > k:\n",
    "                predicted = predicted[:k]\n",
    "            score = 0.0\n",
    "            num_hits = 0.0\n",
    "            for i, p in enumerate(predicted):\n",
    "                if p in actual and p not in predicted[:i]:\n",
    "                    num_hits += 1.0\n",
    "                    score += num_hits / (i + 1.0)\n",
    "            return score / min(len(actual), k) if actual else 0.0\n",
    "\n",
    "        scores = []\n",
    "        for cust, grp in df.groupby(\"customer\"):\n",
    "            actual = grp.loc[grp.y == 1, \"offer\"].tolist()\n",
    "            predicted = grp.sort_values(\"p\", ascending=False)[\"offer\"].tolist()\n",
    "            scores.append(apk(actual, predicted, 7))\n",
    "\n",
    "        return \"MAP@7\", np.mean(scores), True\n",
    "\n",
    "    return map7_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdebdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload filepaths\n",
    "train_data_fp = r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\Revision\\train_df_date.parquet\"\n",
    "test_data_fp = r\"C:\\Users\\siddu\\Desktop\\Decision Science Track\\Revision\\test_df_date.parquet\"\n",
    "\n",
    "# Load files into dataframes\n",
    "train = pd.read_parquet(train_data_fp)\n",
    "test = pd.read_parquet(test_data_fp)\n",
    "\n",
    "# Check for number of rows and columns\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20faa25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[['id4','id12','id13']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b49d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['id4','id12','id13']:\n",
    "    if col in train.columns:\n",
    "        train[col] = pd.to_datetime(train[col], errors='coerce').astype('int64') // 10**9\n",
    "    if col in test.columns:\n",
    "        test[col] = pd.to_datetime(test[col], errors='coerce').astype('int64') // 10**9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS = ['id1','id2','id3','id4','id5','id6', 'id7', 'id8', 'id12','id13','id9', 'id10', 'id11','f378','f374','y']\n",
    "\n",
    "all_cols = list(train.columns)\n",
    "\n",
    "FEATURES = [\n",
    "    c for c in all_cols \n",
    "    if c not in DROP_COLS\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# --- 1. DEFINE YOUR FEATURES LIST ---\n",
    "# !!! REPLACE THIS with your actual FEATURES list !!!\n",
    "FEATURES = [\n",
    "    c for c in all_cols \n",
    "    if c not in DROP_COLS\n",
    "]\n",
    "\n",
    "# --- 2. CONVERT DATETIME COLUMNS ---\n",
    "# This converts 'id5' (and others) to a number (Unix timestamp)\n",
    "datetime_cols = ['id4', 'id5', 'id12', 'id13']\n",
    "for col in datetime_cols:\n",
    "    if col in train.columns:\n",
    "        train[col] = pd.to_datetime(train[col], errors='coerce').astype('int64') // 10**9\n",
    "    if col in test.columns:\n",
    "        test[col] = pd.to_datetime(test[col], errors='coerce').astype('int64') // 10**9\n",
    "\n",
    "# --- 3. CONVERT CATEGORICAL COLUMNS ---\n",
    "# This tells LightGBM to treat these as categories, not numbers\n",
    "categorical_cols = [\n",
    "    'f42', 'f48', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', \n",
    "    'f354', 'id10', 'id8'\n",
    "]\n",
    "for col in categorical_cols:\n",
    "    if col in train.columns:\n",
    "        # Using .astype('category') is the standard way\n",
    "        train[col] = train[col].astype('category')\n",
    "    if col in test.columns:\n",
    "        test[col] = test[col].astype('category')\n",
    "\n",
    "# --- 4. PRE-TRAINING CHECK ---\n",
    "# We check for any columns that are NOT numeric and NOT category\n",
    "numeric_cols = train[FEATURES].select_dtypes(include=np.number).columns\n",
    "category_cols_in_features = [col for col in FEATURES if col in train.columns and train[col].dtype == 'category']\n",
    "\n",
    "# All columns must be either in numeric_cols or category_cols_in_features\n",
    "processed_cols = list(numeric_cols) + category_cols_in_features\n",
    "unhandled_cols = [col for col in FEATURES if col not in processed_cols]\n",
    "\n",
    "if unhandled_cols:\n",
    "    print(\"Error: The following columns in FEATURES are still not numeric or category:\")\n",
    "    print(unhandled_cols)\n",
    "    print(\"Please convert them or remove them from the FEATURES list.\")\n",
    "else:\n",
    "    print(\"All feature columns are numeric or category. Proceeding to training.\")\n",
    "    \n",
    "    # --- 5. START TRAINING LOOP (NOW SAFELY INSIDE 'ELSE' BLOCK) ---\n",
    "    N_SPLITS = 5\n",
    "    gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "    oof_step1 = np.zeros(len(train))\n",
    "    models_step1 = []\n",
    "\n",
    "    # Filter FEATURES to only include columns that exist in the train set\n",
    "    # This prevents errors if a feature in the list was dropped\n",
    "    final_features = [col for col in FEATURES if col in train.columns]\n",
    "    print(f\"Training on {len(final_features)} features.\")\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(gkf.split(train, groups=train['id2'])):\n",
    "        print(\"Fold:\", fold)\n",
    "        \n",
    "        tr = train.iloc[tr_idx]\n",
    "        va = train.iloc[val_idx]\n",
    "\n",
    "        # LightGBM will automatically handle the .astype('category') columns\n",
    "        dtr = lgb.Dataset(tr[final_features], tr['y'])\n",
    "        dva = lgb.Dataset(va[final_features], va['y'], reference=dtr)\n",
    "\n",
    "        params = {\n",
    "            'objective':'binary',\n",
    "            'learning_rate':0.04,\n",
    "            'num_leaves':63,\n",
    "            'min_data_in_leaf':30,\n",
    "            'feature_fraction':0.8,\n",
    "            'bagging_fraction':0.8,\n",
    "            'bagging_freq':5,\n",
    "            'seed':fold + 11,\n",
    "            'verbosity':-1\n",
    "        }\n",
    "\n",
    "        m = lgb.train(\n",
    "            params,\n",
    "            dtr,\n",
    "            valid_sets=[dva],\n",
    "            num_boost_round=2000,\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(100),\n",
    "                lgb.log_evaluation(50)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        oof_step1[val_idx] = m.predict(va[final_features], num_iteration=m.best_iteration)\n",
    "        models_step1.append(m)\n",
    "\n",
    "    train['retrieval_pred'] = oof_step1\n",
    "    test['retrieval_pred'] = np.mean([m.predict(test[final_features]) for m in models_step1], axis=0)\n",
    "    \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(\"OOF predictions added as 'retrieval_pred' to train.\")\n",
    "    print(\"Test predictions added as 'retrieval_pred' to test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a623c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 30\n",
    "\n",
    "cand_train = (\n",
    "    train\n",
    "    .sort_values(['id2','retrieval_pred'], ascending=[True, False])\n",
    "    .groupby('id2')\n",
    "    .head(K)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "cand_test = (\n",
    "    test\n",
    "    .sort_values(['id2','retrieval_pred'], ascending=[True, False])\n",
    "    .groupby('id2')\n",
    "    .head(K)\n",
    "    .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'final_features' is the list of good features from your first stage (cell 12)\n",
    "# We just add the new feature we created in the first stage\n",
    "features_rerank = final_features + ['retrieval_pred']\n",
    "\n",
    "# Enforce intersection just to be safe\n",
    "features_rerank = [\n",
    "    c for c in features_rerank \n",
    "    if c in cand_train.columns and c in cand_test.columns\n",
    "]\n",
    "\n",
    "print(f\"Reranking using {len(features_rerank)} features.\")\n",
    "\n",
    "models_rerank = []\n",
    "oof_rerank = np.zeros(len(cand_train))\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(gkf.split(cand_train, groups=cand_train['id2'])):\n",
    "    print(\"Rerank fold:\", fold)\n",
    "    \n",
    "    tr = cand_train.iloc[tr_idx].reset_index(drop=True)\n",
    "    va = cand_train.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # --- 1. (THE FIX) Create group arrays for the ranker ---\n",
    "    # This tells the ranker how many candidates each customer has\n",
    "    tr_groups = tr.groupby('id2').size().to_numpy()\n",
    "    va_groups = va.groupby('id2').size().to_numpy()\n",
    "\n",
    "    # --- 2. (THE FIX) Add the 'group' parameter to lgb.Dataset ---\n",
    "    dtr = lgb.Dataset(tr[features_rerank], tr['y'], group=tr_groups)\n",
    "    dva = lgb.Dataset(va[features_rerank], va['y'], reference=dtr, group=va_groups)\n",
    "\n",
    "    # --- 3. (THE FIX) Change parameters to be a RANKER ---\n",
    "    params_r = {\n",
    "        'objective': 'lambdarank',  # Use 'lambdarank' for ranking\n",
    "        'metric': 'map',            # Use 'map' as the built-in metric\n",
    "        'eval_at': [7],             # Tell it to calculate MAP@7\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 63,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbosity': -1,\n",
    "        'seed': fold + 42\n",
    "    }\n",
    "\n",
    "    m = lgb.train(\n",
    "        params_r,\n",
    "        dtr,\n",
    "        valid_sets=[dva],\n",
    "        # --- 4. (THE FIX) REMOVE the slow 'feval' parameter ---\n",
    "        # feval=feval,\n",
    "        num_boost_round=2000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(100),\n",
    "            lgb.log_evaluation(50)  # This will now print 'map@7' very quickly\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    oof_rerank[val_idx] = m.predict(va[features_rerank], num_iteration=m.best_iteration)\n",
    "    models_rerank.append(m)\n",
    "\n",
    "cand_train['rerank_pred'] = oof_rerank\n",
    "cand_test['rerank_pred'] = np.mean(\n",
    "    [m.predict(cand_test[features_rerank]) for m in models_rerank],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Rerank Training Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
